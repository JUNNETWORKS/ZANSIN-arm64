# Environment
```
Python 3.8.2
```


# Execution Procedure
Construct the game server (`mini4-game-api`) and start the crawler following these steps:

 1. Clone the repository  
 `~$ git clone https://github.com/ZANSIN-sec/ZANSIN.git`  
 2. Start the game server  
 `~$ cd ./mini4-game-api/`  
 `~/mini4-game-api$ docker-compose up -d`
 3. Install libraries  
 `~/mini4-corporate$ cd ../mini4-crawler/`  
 `~/mini4-crawler$ pip3 install -r requirements.txt`
 4. Execute the crawler  
 `~/mini4-crawler$ python3 main.py --team team-a`  
 ※For debugging, any string can be used as the team name.

Automatically executes single-threaded access to the corporate site and multi-threaded gameplay, continuing until the `set timer ends` or `forced termination (Ctrl+C)`.  
The play status can be checked through the crawler's Local DB or history logs (see `How to` for details).

## To crawl individual systems
The crawler's destination can be changed using the option:  
* `--game`: Crawl the Game only.  
  `~/mini4-crawler$ python3 main.py --team team-a --game`

Combine destinations by specifying each option.

# How to  
## Initializing Local DB
Adding the option `--delete-db` when executing the crawler will initialize the Local DB (`./sqlite3/minih_v4_{team_name}.db`).  
Command example: `~/mini4-crawler$ python3 main.py --team team-a --delete-db`  

Note that users recorded in the Local DB will also be removed from the server-side DB via the withdrawal API.

## Launch in Debug Mode
When testing in a local environment, it's necessary to switch the connection destinations like the game API and score server. Adding the option `--debug` during crawler execution fetches debug connection settings from `config.ini`.  
Command example: `~/mini4-crawler$ python3 main.py --team team-a --debug`  

Note that the debug connection settings are defined in the `config.ini` file under the `~_debug` parameter.

## Timer Setting
The crawler's start and end execution can be controlled with a timer.  
Set the timer in the `config.ini` file under the `[Common]` section with the following parameters:

|Parameter Name|Description|Example|
|:--|:--|:--|
|competition_start_time|Crawling start time (yyyyMMddHHmmSS).|`20201005103000`|
|competition_lunch_time|Lunch break start time (yyyyMMddHHmmSS).|`2020100512000`|
|competition_restart_time|Competition restart time (yyyyMMddHHmmSS).|`2020100513000`|
|competition_busy_time|Crawling acceleration start time (yyyyMMddHHmmSS). Crawling speed doubles from this time until the end.|`20201005153000`|
|competition_end_time|Crawling end time (yyyyMMddHHmmSS).|`20201005160000`|

Since the timer **checks the system clock where the crawler operates**, ensure the system time is correct when using the timer.

## Modifying Game API URL
Change the `url` parameter in each API section of `config.ini` to modify the URL.  
  * User Creation: `[API_NewUser]`  
  * Login: `[API_Login]`  
  * Get User ID: `[API_GetUserId]`  
  * Image Upload: `[API_Upload]` (not used in crawler)  
  * Withdrawal: `[API_Delete]`  
  * Course List Retrieval: `[API_Course]`  
  * Course Selection: `[API_CoursePost]`  
  * Battle: `[API_Battle]`  
  * Stamina Recovery: `[API_Recovery]`  
  * Gacha: `[API_Gatya]`  
  * Get Player Data: `[API_Player]`  
  * Charge: `[API_Charge]`  
  * Ranking: `[API_Ranking]`  

## Modifying Player Count
Default is `1`  
To change, modify the `max_player_num` parameter in the `[Common]` section of `config.ini`.  

## Local Proxy Setting  
To inspect the communication between the crawler and APIs, set a Local Proxy.  
Input the Proxy address in the `proxy_addr` parameter under the `[Common]` section of `config.ini`.  
Example: `http://127.0.0.1:8088`  

## Crawler's Local DB
The crawler's Local DB (SQLite3) is stored at:  
`{crawler root}/sqlite3/mini_v4.db`  

Currently, it only contains the `UserInfoTBL` table for managing user status.  

|Column Name|Description|
|:--|:--|
|id|Unique ID serially generated by the crawler. Asynchronous with the API's id.|
|status|User's active state (1: active user, 0: withdrawn user)|
|charge|User's charge amount (real money)|
|injustice_num|Count of unfair practices. Ban after 3 offenses.|
|user_id|User ID.|
|password|Password.|
|nickname|Nickname (unused by crawler).|
|created_at|User creation time (unused by crawler).|
|level|Level.|
|exp|Experience points.|
|gold|Gold.|
|max_hp|Maximum HP.|
|max_stamina|Maximum Stamina.|
|max_str|Maximum strength (unused by crawler).|
|need_exp|Needed experience (unused by crawler).|
|stamina|Current Stamina.|
|staminaupdated_at|Stamina update time (unused by crawler).|
|weapon_id|Weapon ID (unused by crawler).|
|armor_id|Armor ID (unused by crawler).|

## Checking Play History
Each user's information is appended to a CSV file every epoch when the crawler runs.  
`{crawler root}/history.csv`  

The progression of levels and experience points can be monitored.  

|Column Name|Description|
|:--|:--|
|epoch|Epoch number.|
|user_id|User ID.|
|level|Level.|
|exp|Experience points.|
|gold|Gold.|
|max_stamina|Maximum Stamina.|
|stamina|Current Stamina.|
|weapon_id|Weapon ID (unused by crawler).|
|armor_id|Armor ID (unused by crawler).|
|charge_sum|User's total charge amount (real money).|
|date|Record creation time.|

## Log Inspection
Each player's execution history is recorded.  
`{crawler root}/logs/mini-crawler_{team_name}.log`  

Players are identified in the log by the `id`.  
※The `id` refers to that within the crawler's Local DB and is asynchronous with the API's id.

That's all.
